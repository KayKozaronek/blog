{
  
    
        "post0": {
            "title": "A fun short story that never happened",
            "content": "A fun short story that never happened . The Basics: . #TODO Explain the “Wing It” game and it’s rules!!! . Time used: . 35 minutes | 11.07.22 | . Situation: . You are a kindergarten teacher at a very exclusive private elementary school. While you were in the bathroom, a little girl used a magnifying glass to ignite a stack of papers, and now all three floors of the building are on fire and full of precious children. The elevators have shut down, and there’s heavy smoke in the staircase. . Resources: . Eight boxes of baking powder (soda), assorted brands | 20%-off coupon to Bed, Bath and Beyond | Parachute | Helicopter | Hang glider | Story: . The other day I learned something extraordinary about the safety features of baking soda, parachutes, and coupons. It was a regular Monday morning at the kindergarten when it struck 12:13 o’clock. That’s when I usually take a coffee break. That is, not a break to drink coffee but rather a break to get rid of the delicate side products the coffee makes me produce. Here’s something you need to know: I really like to squeeze out every ounce of my coffee break. Wait, that came out the wrong way. Man, somehow that too. I’m on a run!! Anyhow, I took my time enjoying the breaking news about how something funny like any self-respecting teacher does when my nose detected a rather discomforting smell. And no, I’m not talking about my wonderful coffee side product. It was a rather smokey smell, which is weird, because we usually don’t schedule our weekly fart lighting contest until Thursday. In short succession I heard multiple people break out in tears and cry. Again, highly unusual, since my colleague Robert usually only cries on Friday when it’s time to say goodbye to his favorite children. Moreover, he was on vacation. . As you can see, I got seriously confused. So I pulled up my pants as fast as possible and left the toilet to see what was going on. And no, I have to disappoint you, I did not forget to mention the part where I whipped my tender buttox. . Leaving the loo, I turned around and discovered that a little girl named Lilly used a magnifying glass to ignite a stack of papers, and now all three floors of the building were on fire and full of precious children. You surely understand my surprise when I saw this. Just last week I was about to send Lily’s parents a letter to notify them about her poor performance in science class and yet here she was, applying a magnifying glass to foster a world class inferno. I was proud, to say the very least! Lilly finally learned the fine arts of combustion, I’m an awesome teacher: Check! I know, I know. You’re probably thinking: “Science class in kindergarten” that’s nuts? Well, I absolutely agree, these overly ambitious helicopter parents simply find more and more ways to get their kids to become the next Elon Musk. Not that it would bother me. I’m quite glad actually. These little brats are the perfect audience for the lonesome PhD archetype that I am. I mean, common guys: I can geek around about sciency stuff all day long without having to fear any intellectual retaliation. They don’t ask any pesky questions and just nod, as if they understood. They are so much better than most of the students I’ve worked with over the years. It’s a win win. Putting aside the important stuff, we can redirect our attention to that tini tiny flame I mentioned earlier. Right, so there’s this huge fire, bunches of screaming children and a brilliant 3 year pyromaniac. So I’m thinking: my main priority is to get everybody out of the building. Well, everyone except for nosy Nate. That tiny monster recently attacked me with a specialty of his: boggers and snot soup. You can probably guess why he’s been giving the rather accurate nickname. Now I’d like to invite you to put yourself into my shoes: “Your’re on the pre to last floor and it seems like all ways down are blocked, the only rooms on this floor are the kitchen, the toilet and the super-duper shut-eye slumber sanctuary.” Yes, you heard me right, imagine saying this 3 times a day to get 30 tiny fuckers up to the 3rd floor of the building: “Hey hey you little sweethearts, it’s time to frequent the super-duper shut-eye slumber sanctuary”. I always said, we shouldn’t have appointed Kathrin to “Supreme Room Name Selector”. First of all, after she was done with her duties – on the first day of work, who could’ve imagined – it wasn’t long before she started naming other things. Then again, we have too much money anyway and Kathrin doesn’t have any talents so doing the math on this one, I think we’re fine. At the very least we’re better off because Kathrine is not let loose on the children. . Where was I? Right, that’s the wrong question. Where were you? Now that you can see in what quandary I found myself, try and ask yourself what you’d do next? Correct: “Baking powder”. That was my first instinct as well. Oh, why baking powder? I’m glad that you asked. According to Wikipedia, backing powder has an amazing ability to extinguish fires. At least that’s what I learned sitting on the toilet preparing my next science lesson. 30 kids, a tiny flame and Eight boxes of baking powder assorted brands of course, that was gonna be a hit lecture. What I didn’t know is that the flame would be substituted by a raging hell fire, but as every great teacher, I had to rise to the occasion. Besides, the hotter the flame, the starker the effect. Every good magician knows that. Although the good ones probably stay professional and call their assistants something else. Right as I was about to unpack one of the boxes of backing powder my group of kids froze in a sudden shock of realization. Some started pointing at the backing powder and started waiving their hands frantically. Taking this as a sign that they were ready for the magic, I opened up a pack and threw it in the fire. Fireworks, combustion, surprise and disgust. Those were the emotions I felt when I realized, that I’d misread the Wikipedia article. It wasn’t baking powder that extinguished fire, but rather baking soda. The powder itself is highly combustible and created an immediate shockwave of heat and disillusionment in my pupils: “Their bellowed teacher made a fatal mistake”. Taken aback by my failure and the loss of eyebrows I endured because of the jet of flame I watched in surprise as my students identified a pack of backing soda in the kitchen. They put out the flame and helped me get back on my feet to climb the staircase and make it out on the rooftop. (Wow, these tiny twats are brilliant – it’s surely not me who taught them that.) From here on it was a breeze. The kids and I were finally free, breathing some fresh air, taking in the beautiful view from our luxurious penthouse kindergarten in the Hills of Los MoneyCanBuyAnythingExceptSmartKindergartenTeachers. In a short instance we forgot that the building was crumbling underneath our feet which is why we had to act quickly. Fortunately, enough we were not alone on the rooftop. A beautiful black M-372 pantera helicopter was waiting for me to mount it. You’re probably thinking: “A helicopter in a kindergarten, yeah, as if.“ I mean yeah, we’re loaded. I did mention we’re a private kindergarten right? Damn peasants. Anyway, a couple of weeks later it’s time for the big moment. I’m getting my medal of honor. At least that’s what I was thinking. We’re standing in the majors hall. Suited up and freshly bathed I’m thinking to myself: Damn today’s the day, you’re looking hella good in your high school suit. But to be honest I possibly looked kinda funny with one quarter of an eyebrow. Anyway, my mom said that I would never need the suit again, but here I was, showing here wrong. Isn’t that what it’s all about? Instead of the long awaited medal of honor I’m given a small box with something marvelous inside. It’s a 20%-off coupon to Bed, Bath and Beyond .",
            "url": "https://kaykozaronek.github.io/blog/wing%20it/extreme%20storytelling/experiment/2022/12/07/Short-Story.html",
            "relUrl": "/wing%20it/extreme%20storytelling/experiment/2022/12/07/Short-Story.html",
            "date": " • Dec 7, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Exercise - Encoder Decoder Transformer",
            "content": "&quot;This exercise guides you to a full implementation of a transformer with Pytorch and HuggingFace&quot; . Imports . from transformers import AutoTokenizer from transformers import AutoModel from transformers import AutoConfig from torch import nn import torch.nn.functional as F import torch from math import sqrt . Tokenizer . Exercise: . Load the tokenizer for bert-base-uncased | Print the input ids of the text: &quot;time flies like an arrow&quot; | . Your Code . Solution . #collapse-output model_ckpt = &quot;bert-base-uncased&quot; tokenizer = AutoTokenizer.from_pretrained(model_ckpt) text = &quot;time flies like an arrow&quot; inputs = tokenizer(text, return_tensors=&quot;pt&quot;, add_special_tokens=False) inputs.input_ids . . tensor([[ 2051, 10029, 2066, 2019, 8612]]) . Model . Exercise: . Load the pretrained model | Take a look at the architecture | . Your Code . Solution . #collapse-output model = AutoModel.from_pretrained(model_ckpt) model . . Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: [&#39;cls.seq_relationship.weight&#39;, &#39;cls.predictions.bias&#39;, &#39;cls.predictions.transform.dense.weight&#39;, &#39;cls.predictions.transform.dense.bias&#39;, &#39;cls.seq_relationship.bias&#39;, &#39;cls.predictions.transform.LayerNorm.weight&#39;, &#39;cls.predictions.transform.LayerNorm.bias&#39;, &#39;cls.predictions.decoder.weight&#39;] - This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model). - This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model). . BertModel( (embeddings): BertEmbeddings( (word_embeddings): Embedding(30522, 768, padding_idx=0) (position_embeddings): Embedding(512, 768) (token_type_embeddings): Embedding(2, 768) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) (encoder): BertEncoder( (layer): ModuleList( (0): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) (intermediate_act_fn): GELUActivation() ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (1): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) (intermediate_act_fn): GELUActivation() ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (2): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) (intermediate_act_fn): GELUActivation() ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (3): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) (intermediate_act_fn): GELUActivation() ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (4): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) (intermediate_act_fn): GELUActivation() ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (5): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) (intermediate_act_fn): GELUActivation() ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (6): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) (intermediate_act_fn): GELUActivation() ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (7): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) (intermediate_act_fn): GELUActivation() ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (8): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) (intermediate_act_fn): GELUActivation() ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (9): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) (intermediate_act_fn): GELUActivation() ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (10): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) (intermediate_act_fn): GELUActivation() ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (11): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) (intermediate_act_fn): GELUActivation() ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) ) ) (pooler): BertPooler( (dense): Linear(in_features=768, out_features=768, bias=True) (activation): Tanh() ) ) . Config . Exercise: . Load the configuration file | Take a look at the config object | Find the hidden size of the model | . Your Code . Solution . config = AutoConfig.from_pretrained(model_ckpt) config . . BertConfig { &#34;_name_or_path&#34;: &#34;bert-base-uncased&#34;, &#34;architectures&#34;: [ &#34;BertForMaskedLM&#34; ], &#34;attention_probs_dropout_prob&#34;: 0.1, &#34;classifier_dropout&#34;: null, &#34;gradient_checkpointing&#34;: false, &#34;hidden_act&#34;: &#34;gelu&#34;, &#34;hidden_dropout_prob&#34;: 0.1, &#34;hidden_size&#34;: 768, &#34;initializer_range&#34;: 0.02, &#34;intermediate_size&#34;: 3072, &#34;layer_norm_eps&#34;: 1e-12, &#34;max_position_embeddings&#34;: 512, &#34;model_type&#34;: &#34;bert&#34;, &#34;num_attention_heads&#34;: 12, &#34;num_hidden_layers&#34;: 12, &#34;pad_token_id&#34;: 0, &#34;position_embedding_type&#34;: &#34;absolute&#34;, &#34;transformers_version&#34;: &#34;4.20.1&#34;, &#34;type_vocab_size&#34;: 2, &#34;use_cache&#34;: true, &#34;vocab_size&#34;: 30522 } . Self Attention . Exercise: . Write the scaled_dot_product_attentionfunction | . . Your Code . def scaled_dot_product_attention(): &quot;&quot;&quot;Example function with types documented in the docstring. Args: query (torch.tensor): Query tensor. key (torch.tensor): Key tensor. value (torch.tensor): Value tensor. Returns: attention (torch.tensor): Scaled dot product attention. &quot;&quot;&quot; pass . Solution . def scaled_dot_product_attention(query, key, value): &quot;&quot;&quot;Example function with types documented in the docstring. Args: query (torch.tensor): Query tensor. key (torch.tensor): Key tensor. value (torch.tensor): Value tensor. Returns: attention (torch.tensor): Scaled dot product attention. &quot;&quot;&quot; dim_k = query.size(-1) scores = torch.bmm(query, key.transpose(1, 2)) / sqrt(dim_k) weights = F.softmax(scores, dim=-1) return torch.bmm(weights, value) . . Attention Head . Your Code . class AttentionHead(nn.Module): &quot;&quot;&quot; Implements a single attention head. Multiple heads can be used in Multi-Head Attention. &quot;&quot;&quot; def __init__(self): pass def forward(self): pass . . Solution . class AttentionHead(nn.Module): &quot;&quot;&quot; Implements a single attention head. Multiple heads can be used in Multi-Head Attention. &quot;&quot;&quot; def __init__(self, embed_dim, head_dim): super().__init__() self.q = nn.Linear(embed_dim, head_dim) self.k = nn.Linear(embed_dim, head_dim) self.v = nn.Linear(embed_dim, head_dim) def forward(self, hidden_state): attn_outputs = scaled_dot_product_attention( self.q(hidden_state), self.k(hidden_state), self.v(hidden_state)) return attn_outputs . . Multi-Head Attention . Your Code . class MultiHeadAttention(nn.Module): &quot;&quot;&quot;Creates Multi-Head Attention Layer by determining embedding dimension, number of heads and head dimension from the config file. &quot;&quot;&quot; def __init__(self): pass def forward(self): pass . . Solution . class MultiHeadAttention(nn.Module): &quot;&quot;&quot;Creates Multi-Head Attention Layer by determining embedding dimension, number of heads and head dimension from the config file. &quot;&quot;&quot; def __init__(self, config): super().__init__() embed_dim = config.hidden_size num_heads = config.num_attention_heads head_dim = embed_dim // num_heads self.heads = nn.ModuleList( [AttentionHead(embed_dim, head_dim) for _ in range(num_heads)] ) self.output_linear = nn.Linear(embed_dim, embed_dim) def forward(self, hidden_state): x = torch.cat([h(hidden_state) for h in self.heads], dim=-1) x = self.output_linear(x) return x . . Feed Forward . Your Code . class FeedForward(nn.Module): &quot;&quot;&quot;Implements feed forward layer with GeLU and Dropout. Contains 2 Linear layers. &quot;&quot;&quot; def __init__(self): pass def forward(self): pass . Solution . class FeedForward(nn.Module): &quot;&quot;&quot;Implements feed forward layer with GeLU and Dropout. Contains 2 Linear layers. &quot;&quot;&quot; def __init__(self, config): super().__init__() self.linear_1 = nn.Linear(config.hidden_size, config.intermediate_size) self.linear_2 = nn.Linear(config.intermediate_size, config.hidden_size) self.gelu = nn.GELU() self.dropout = nn.Dropout(config.hidden_dropout_prob) def forward(self, x): x = self.linear_1(x) x = self.gelu(x) x = self.linear_2(x) x = self.dropout(x) return x . . Encoder Layer . Your Code . class TransformerEncoderLayer(nn.Module): &quot;&quot;&quot; Combines Multi-Head Attention with Layer Normalization, Feed Forward Layer and skip connections to obtain an Encoder Layer. &quot;&quot;&quot; def __init__(self): pass def forward(self): pass . Solution . class TransformerEncoderLayer(nn.Module): &quot;&quot;&quot; Combines Multi-Head Attention with Layer Normalization, Feed Forward Layer and skip connections to obtain an Encoder Layer. &quot;&quot;&quot; def __init__(self, config): super().__init__() self.layer_norm_1 = nn.LayerNorm(config.hidden_size) self.layer_norm_2 = nn.LayerNorm(config.hidden_size) self.attention = MultiHeadAttention(config) self.feed_forward = FeedForward(config) def forward(self, x): # Apply layer normalization and then copy input into query, key, value hidden_state = self.layer_norm_1(x) # Apply attention with a skip connection x = x + self.attention(hidden_state) # Apply feed-forward layer with a skip connection x = x + self.feed_forward(self.layer_norm_2(x)) return x . . Embeddings . Your Code . class Embeddings(nn.Module): &quot;&quot;&quot;Implements token embeddings and positional embbeddings. &quot;&quot;&quot; def __init__(self): pass def forward(self): pass . . Solution . class Embeddings(nn.Module): &quot;&quot;&quot;Implements token embeddings and positional embbeddings. &quot;&quot;&quot; def __init__(self, config): super().__init__() self.token_embeddings = nn.Embedding(config.vocab_size, config.hidden_size) self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size) self.layer_norm = nn.LayerNorm(config.hidden_size, eps=1e-12) self.dropout = nn.Dropout() def forward(self, input_ids): # Create position IDs for input sequence seq_length = input_ids.size(1) position_ids = torch.arange(seq_length, dtype=torch.long).unsqueeze(0) # Create token and position embeddings token_embeddings = self.token_embeddings(input_ids) position_embeddings = self.position_embeddings(position_ids) # Combine token and position embeddings embeddings = token_embeddings + position_embeddings embeddings = self.layer_norm(embeddings) embeddings = self.dropout(embeddings) return embeddings . . Transformer Encoder . Your Code . class TransformerEncoder(nn.Module): &quot;&quot;&quot;Combines the Embedding layer and multiple encoder layers to obtain full transformer encoder. &quot;&quot;&quot; def __init__(self): pass def forward(self): pass . Solution . class TransformerEncoder(nn.Module): &quot;&quot;&quot;Combines the Embedding layer and multiple encoder layers to obtain full transformer encoder. &quot;&quot;&quot; def __init__(self, config): super().__init__() self.embeddings = Embeddings(config) self.layers = nn.ModuleList( [TransformerEncoderLayer(config) for _ in range(config.num_hidden_layers)]) def forward(self, x): x = self.embeddings(x) for layer in self.layers: x = layer(x) return x . . Classification Head . Your Code . class TransformerForSequenceClassification(nn.Module): def __init__(self): pass def forward(self): pass . . Solution . class TransformerForSequenceClassification(nn.Module): def __init__(self, config): super().__init__() self.encoder = TransformerEncoder(config) self.dropout = nn.Dropout(config.hidden_dropout_prob) self.classifier = nn.Linear(config.hidden_size, config.num_labels) def forward(self, x): x = self.encoder(x)[:, 0, :] # select hidden state of [CLS] token x = self.dropout(x) x = self.classifier(x) return x . . Masked Attention . Exercise: . Change the scaled_dot_product_attention such that it can incorporate a mask. | . Your Code . def scaled_dot_product_attention(): pass . . Solution . def scaled_dot_product_attention(query, key, value, mask=None): dim_k = query.size(-1) scores = torch.bmm(query, key.transpose(1, 2)) / sqrt(dim_k) if mask is not None: scores = scores.masked_fill(mask == 0, float(&quot;-inf&quot;)) weights = F.softmax(scores, dim=-1) return weights.bmm(value) . . Putting it all together: Encoder-Decoder Transformer . Your Code . Solution . . .",
            "url": "https://kaykozaronek.github.io/blog/attention%20is%20all%20you%20need/transformer/pytorch/nlp/coding%20exercise/2022/07/11/Exercise-Transformers.html",
            "relUrl": "/attention%20is%20all%20you%20need/transformer/pytorch/nlp/coding%20exercise/2022/07/11/Exercise-Transformers.html",
            "date": " • Jul 11, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "Weekly Goals Update 03",
            "content": "Status: . Last week was great. I met a bunch of very intelligent, ambitious and intelectually curious people. The CHERI summer fellowship seems promising. I set up an accountability partners group, communicated my mentorship expectations and made good friends. . Goal Points Progress Total Points . 0 CHERI Introductory Week | 70 | 1.000000 | 70.000000 | . 1 Catch up with Anki | 5 | 1.000000 | 5.000000 | . 2 Read AGI Safety Fundamentals from first principles | 15 | 0.000000 | 0.000000 | . 3 Write down expectations for mentorship | 10 | 1.000000 | 10.000000 | . 4 Total | | | 85.000000 | . Goals: . This week my main goal is to reimplement the &quot;Training Language Models with Language Feedback&quot; paper. To do so, I will have to acquaint myself with the OpenAI API. . As a secondary goal, I want to work through the NYU NLP lecture and start watching the Deep RL Lecture Series (UC Berkeley). . Finally, it&#39;d be fun to check out the local EA chapter in Budapest. . At the end of the week I hope to converge on a stable weekly schedule I can follow throughout the summer fellowship. . Goal Points . 0 Work through OpenAI API guides | 30 | . 1 Reimplement &quot;training language models with lan... | 25 | . 2 Publish Transformer Exercise | 15 | . 3 NYU Deep Learning Course Week 12 | 20 | . 4 Attend EA meeting in Budapest | 5 | . 5 Create standard weekly schedule | 5 | . 6 Bonus: Check out Deep RL course | 5 | . 7 Total | | . Explore in upcoming week(s): . Mastering Pytorch | .",
            "url": "https://kaykozaronek.github.io/blog/reading/books/academic%20papers/blogs/mental%20diet/2022/07/10/Update-03-Weekly-Goals.html",
            "relUrl": "/reading/books/academic%20papers/blogs/mental%20diet/2022/07/10/Update-03-Weekly-Goals.html",
            "date": " • Jul 10, 2022"
        }
        
    
  
    
        ,"post3": {
            "title": "Weekly Goals Update 02",
            "content": "Status: . Here&#39;s the progress of last weeks goals. . Goal Points Progress Total Points . 0 Reimplement &quot;Training Language Models with Language Feedback&quot; | 25 | 0.200000 | 5.000000 | . 1 Finish Software Engineering practices for Data Scientists | 15 | 1.000000 | 15.000000 | . 2 Create blog | 25 | 1.000000 | 25.000000 | . 3 Apply to Future Forum | 10 | 1.000000 | 10.000000 | . 4 Apply to Future Academy | 5 | 1.000000 | 5.000000 | . 5 Code Proximal Policy Optimization (PPO) | 5 | 0.500000 | 2.500000 | . 6 Code Generalized Advantage Estimation (GAE) | 5 | 1.000000 | 5.000000 | . 7 Creat Github repo for CHERI project | 5 | 0.000000 | 0.000000 | . 8 Do CHERI project planning | 5 | 1.000000 | 5.000000 | . 9 Total | | | 72.500000 | . Goals: . This week is non representative. It&#39;s main focus is not on advancing my project, but rather getting it started and setting up the right procedures. A key focus of this week is to correctly establish my goals, the mentorship relationship and the &quot;&gt; 90%-final&quot; project plan. Besides that, there are a lot of prescheduled events by the CHERI organizers in which I will participate. The focus will be in building stong ties with my peers and ask them the Hamming question: “What are the important problems in your field, and why are(n&#39;t) you working on them? . Goal Points . 0 CHERI Introductory Week | 70 | . 1 Catch up with Anki | 5 | . 2 Read AGI Safety Fundamentals from first princi... | 15 | . 3 Write down expectations for mentorship | 10 | . 4 Total | | . Explore in upcoming week(s): . Explore OpenAI API | Deep RL Lecture Series (UC Berkeley) | NYU Pytorch Deep Learning | Mastering Pytorch | .",
            "url": "https://kaykozaronek.github.io/blog/reading/books/academic%20papers/blogs/mental%20diet/2022/07/03/Update-02-Weekly-Goals.html",
            "relUrl": "/reading/books/academic%20papers/blogs/mental%20diet/2022/07/03/Update-02-Weekly-Goals.html",
            "date": " • Jul 3, 2022"
        }
        
    
  
    
        ,"post4": {
            "title": "Reading List",
            "content": ". Tip: Feel free to suggest readings in the comment section. . Title Category Topic Progress . 0 Natural Language Processing with Transformers | Book | NLP | 100 | . 1 Mastering PyTorch | Book | Deep Learning | 0 | . 2 Self-critiquing models for assisting human evaluators | Academic Paper | NLP | 30 | . 3 Training Language Models with Language Feedback | Academic Paper | NLP | 50 | . 4 Learning to summarize from human feedback | Academic Paper | NLP | 30 | . 5 AGI Safety From First Principles | Blog | AI Safety | 0 | . 6 Propositions Concerning Digital Minds and Society | Academic Paper | Consciousness, Digital Minds | 50 | .",
            "url": "https://kaykozaronek.github.io/blog/reading/books/academic%20papers/blogs/mental%20diet/2022/07/02/Reading-List.html",
            "relUrl": "/reading/books/academic%20papers/blogs/mental%20diet/2022/07/02/Reading-List.html",
            "date": " • Jul 2, 2022"
        }
        
    
  
    
        ,"post5": {
            "title": "Exploring the OpenAI critiques dataset",
            "content": "Introduction . In this notebook I am taking a look at the new dataset released by OpenAI on the 13th of June in the paper: &quot;Self-critiquing models for assisting human evaluators&quot;. . Imports . from datasets import load_dataset import pandas as pd import matplotlib.pyplot as plt import torch import torch.nn.functional as F . The Data . 1. Base Dataset . The Base Dataset it structured like so: . Train . id | split | time | labeler | is_topic_based_summarization | data passage text | title | . | questions list of multiple question-answer pairs (dictionaries) | . | . | . | Test . etc. | . | . base_url = &quot;https://openaipublic.blob.core.windows.net/critiques/dataset/base/&quot; base_dataset = load_dataset(&#39;json&#39;, data_files={&#39;train&#39;: base_url + &#39;train.jsonl.gz&#39;, &#39;test&#39;: base_url + &#39;test.jsonl.gz&#39;}) base_dataset . Using custom data configuration default-b6bdeb08a451c657 Reusing dataset json (/root/.cache/huggingface/datasets/json/default-b6bdeb08a451c657/0.0.0/da492aad5680612e4028e7f6ddc04b1dfcec4b64db470ed7cc5f2bb265b9b6b5) . DatasetDict({ train: Dataset({ features: [&#39;id&#39;, &#39;split&#39;, &#39;time&#39;, &#39;labeler&#39;, &#39;is_topic_based_summarization&#39;, &#39;data&#39;], num_rows: 11232 }) test: Dataset({ features: [&#39;id&#39;, &#39;split&#39;, &#39;time&#39;, &#39;labeler&#39;, &#39;is_topic_based_summarization&#39;, &#39;data&#39;], num_rows: 3857 }) }) . . Figuring out the structure of the dataset . df = pd.DataFrame(base_dataset[&#39;train&#39;]) df.head() . id split time labeler is_topic_based_summarization data . 0 Hvwa1M6h1l81jmLcI2WdliTDayc6ik | train | 1.654295e+09 | 9d66ba714984b4ac37359c8a26b065d2d5e1d508b349a2... | True | {&#39;passage&#39;: {&#39;text&#39;: &#39;[Between now and the end... | . 1 tCYeeDnFlAqrHk9HHYV5BnC4JD5eYQ | train | 1.654295e+09 | d95e9d66406f3756657b3e159c883527a54ebe2d11fcb6... | False | {&#39;passage&#39;: {&#39;text&#39;: &#39; The hikers had left a m... | . 2 YfuuobgOYOdMtT7nSuaLFaJ7yRBjX6 | train | 1.654295e+09 | 8774b0664d5c0ab1502c35813c97e6ae44b477c0ac0a7c... | False | {&#39;passage&#39;: {&#39;text&#39;: &#39;CHARLESTON EXECUTIVE AIR... | . 3 b5KtNkpspmPl9Is6uAaDTBAGKFsUZg | train | 1.654295e+09 | c386a07c8ceed1b1dfcd2126015c772310f9ccf5c34a82... | False | {&#39;passage&#39;: {&#39;text&#39;: &#39;I reach into my pocket t... | . 4 08dDPy00rGm3wNIZPusqxSL8A8ADxu | train | 1.654295e+09 | 9217c5bbd255314f3a9222a5c253cc60571b546da6c393... | False | {&#39;passage&#39;: {&#39;text&#39;: &#39;Whatever had wiped out a... | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; df[&#39;data&#39;][0].keys() . dict_keys([&#39;passage&#39;, &#39;questions&#39;]) . df[&#39;data&#39;][0][&#39;passage&#39;].keys() . dict_keys([&#39;text&#39;, &#39;title&#39;]) . df[&#39;data&#39;][0][&#39;passage&#39;][&#39;text&#39;] . &#34;[Between now and the end of the year, Justin Trudeau&#39;s capacity to forge a consensus in Parliament, on the federal-provincial front and, possibly, within his own caucus will be tested as the rubber meets the road on some key campaign commitments, writes Chantal Hébert.] n n[Chantal Hébert] n nChantal HébertNational Affairs Columnist n n__Sat., Nov. 19, 20163 min. read n nArticle was updated Nov. 18, 2016 n nIf you have been enjoying Canada’s comparatively cool political climate since Justin Trudeau became prime minister, make the most of what may be the last days of the season. n nBy all indications, the political temperature is about to rise as deadlines looms on three potentially troublesome fronts for the Liberal government. n nBetween now and the end of the year, the prime minister’s capacity to forge a consensus in Parliament, on the federal-provincial front and, possibly, within his own caucus will be tested as the rubber meets the road on some key campaign commitments. n nOn or before Dec. 1, the special committee that has been exploring a reform of Canada’s voting system will report its findings to the government. If the Liberals have a principled position on this issue, they have been doing a great job of keeping it under wraps. n nThe committee report should signal the beginning of the end of the Liberal game of hide-and-seek. n nThe opposition parties hold the majority at the electoral reform table and, in any event, no government is bound to implement the prescription of a committee. If such an obligation existed, Canada’s new law on medically assisted suicide would be a lot less restrictive. But if Trudeau is presented with an opposition consensus as to the way forward on the voting system he will, at a minimum, have to come up with the kind of coherent response that has been sorely lacking to date. n nThis week, democratic reform minister Maryam Monsef reported, on the basis of her own consultations, that there was no consensus within the public as to a preferred voting system. The representations made to the committee on the other hand have tended to favour a more proportional system. Consensus, in this instance, is very much in the opportunistic eye of the beholder. But more on that later in this column. n nThe odds of a majority committee report increased this week when the NDP signalled that it could support the Conservative call for any new voting system to be put to a national referendum. If there is solid majority within the electorate to be found for anything pertaining to electoral reform, it revolves around the notion that a change should be approved through a national plebiscite. n nOne way or another, it does seem that at least one part of Trudeau’s promise will not be fulfilled. In the still unlikely scenario that the Liberals sign off on a national plebiscite, the debate would shift to the rewriting of the federal referendum law and then to the actual holding of a national vote. Getting all that done within the time frame Elections Canada says it needs to put a different voting system in place for 2019 would be extremely difficult. And that is, of course, assuming a reform proposal wins the day. n nOn Dec. 9, Trudeau is tentatively scheduled to meet with the premiers to put the finishing touches on the country’s climate change strategy. The first ministers have not gathered since the prime minister signalled his intention to set a floor price on carbon. In the interval, Donald Trump’s victory and the expectation that his administration will not follow up on the Paris climate accord have added grist to the mill of opponents of a Canadian carbon tax. n nTrudeau does not lack for provincial allies on carbon pricing but the same is not true of his plan to cut the annual increase of the health transfer to 3 per cent. The prime minister wants to avoid a linkage between the two files. Absent some conciliatory federal move on health-care funding, that linkage may be hard to avoid next month and not just on carbon pricing. n nDec. 19 is the deadline for the federal cabinet to decide the fate of Kinder Morgan’s plan to increase the capacity of the TransMountain pipeline. It links Alberta to the coast off Vancouver. In the wake of the American election, Energy Minister Jim Carr has argued that Trump’s victory and the prospect of a revival of the Keystone XL project did not diminish the need for more pipeline capacity in Canada. n nTrudeau has long said he would not proceed with a pipeline absent a so-called social licence for the project. If his government applied to the quest of a pro-pipeline consensus in British Columbia the same loose criteria it is using to declare that there is no consensus in sight on electoral reform, the TransMountain pipeline would be dead on arrival. n nLoading... n nLoading...Loading...Loading...Loading...Loading... n&#34; . . df[&#39;data&#39;][0][&#39;questions&#39;] . [{&#39;answer&#39;: &#39;&#39;, &#39;question&#39;: &#39;What does the text say about National Affairs Columnist, Chantal Hébert?&#39;}, {&#39;answer&#39;: &#34;The prime minister&#39;s capacity to forge a consensus in Parliament, on the federal-provincial front and his own caucus will be tested as some key campaign commitments face their deadlines. There are three potentially troublesome fronts for Trudeau&#39;s Liberal government. He must come up with a coherent response to the voting system. &#34;, &#39;question&#39;: &#39;What does the text say about Canadian Prime Minister Justin Trudeau?&#39;}, {&#39;answer&#39;: &#34;On or before December 1, the special committee exploring reform of Canada&#39;s voting system will report its findings to the government. Democratic reform minister Maryam Monsef made her own consultations and found that there was no consensus within the public as to a preferred voting system. On December 9, Trudeau is tentatively scheduled to meet with the premiers to put the finishing touches on the country&#39;s climate change strategy. &#34;, &#39;question&#39;: &#39;Summarize everything that happened between December 1 to December 9.&#39;}, {&#39;answer&#39;: &#34;December 19 is the deadline for the federal cabinet to decide the fate of Kinder Morgan&#39;s plan to increase the capacity of the TransMountain pipeline. The pipeline links Alberta to the coast of Vancouver. However, Trudeau has long said that he would not proceed with a pipeline absent a social license for the project.&#34;, &#39;question&#39;: &#39;What does the text say about December 19th?&#39;}] . . Id, split and time . We expect each id to be unique | Since we&#39;re looking at just the training set we expect split to only have 1 unique value: &#39;train&#39; | I don&#39;t know yet how to interpret the time column, but I think it&#39;s not all too important: Could be how long did labeling this piece took | Or date and time at which labeler started labeling | . | . df[&#39;id&#39;].nunique() == len(df) . True . df[&#39;split&#39;].unique() . array([&#39;train&#39;], dtype=object) . df[&#39;time&#39;].nunique() . 11232 . df[&#39;time&#39;][0] . 1654294771.222219 . is_topic_based_summarization . This column tells us whether the summarization is topic based or not | What are the other options though? | . topic_based_df = df[&#39;is_topic_based_summarization&#39;].value_counts() topic_based_df.plot(kind=&#39;bar&#39;) plt.title(&#39;Count of (non) topic based summarizations&#39;) plt.ylabel(&#39;Frequency&#39;) plt.xlabel(&#39;Is topic based summarization&#39;); . print(f&#39;Topic based: {round(topic_based_df[False] / len(df) * 100, 2)} % of the data&#39;) print(f&#39;Not topic based: {round(topic_based_df[True] / len(df) * 100, 2)} % of the data&#39;) . Topic based: 80.23 % of the data Not topic based: 19.77 % of the data . labeler . How many unique labelers were there? | How many items did each labeler label | What was the mean amount each labeler labeled | . num_labelers = len(df[&#39;labeler&#39;].unique()) num_labelers . 35 . rename_labelers = [f&#39;labeler {i}&#39; for i in range(1, num_labelers +1)] len(rename_labelers) . 35 . df = df.replace({&#39;labeler&#39;: dict(zip(df[&#39;labeler&#39;].unique(), rename_labelers))}) df.head(2) . . id split time labeler is_topic_based_summarization data . 0 Hvwa1M6h1l81jmLcI2WdliTDayc6ik | train | 1.654295e+09 | labeler 1 | True | {&#39;passage&#39;: {&#39;text&#39;: &#39;[Between now and the end... | . 1 tCYeeDnFlAqrHk9HHYV5BnC4JD5eYQ | train | 1.654295e+09 | labeler 2 | False | {&#39;passage&#39;: {&#39;text&#39;: &#39; The hikers had left a m... | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; labeler_count_df = df.groupby(&#39;labeler&#39;).count()[&#39;data&#39;] labeler_count_df.sort_values().plot(kind=&#39;barh&#39;, figsize=(5,10)) mean_val = labeler_count_df.mean() plt.axvline(x=mean_val, color=&#39;r&#39;, linestyle=&#39;--&#39;, label=&#39;mean&#39;) plt.title(&#39;Labelers by contribution&#39;) plt.xlabel(&#39;Number of labeled data&#39;) plt.text(mean_val+50, 17,&#39;mean&#39;, c=&#39;red&#39;, rotation=45); . Let&#39;s finally look at our data . data.passage . How long are the texts on average? | How long are the titles on average? | . data = df[&#39;data&#39;] text_len = 0 title_len = 0 for datapoint in data: text_len += len(datapoint[&#39;passage&#39;][&#39;text&#39;]) title_len += len(datapoint[&#39;passage&#39;][&#39;title&#39;]) if datapoint[&#39;passage&#39;][&#39;title&#39;] != None else 0 print(f&#39;Mean text length: {int(text_len / len(data))} words&#39;) print(f&#39;Mean title length: {int(title_len / len(data))} words&#39;) . Mean text length: 4563 words Mean title length: 19 words . print(data[1][&#39;passage&#39;][&#39;text&#39;][:100]) . The hikers had left a map of the area which gave us a vast layout of the land. It wasn&#39;t good. Ther . data[1][&#39;passage&#39;][&#39;title&#39;] . &#39;The Walking Dead: Winter Chronicles nChapter Four: Five Sisters&#39; . data.questions . How do the answers and qustions look like? | How many question answer pairs are there per datapoint? | . df[&#39;data&#39;][0][&#39;questions&#39;] . [{&#39;answer&#39;: &#39;&#39;, &#39;question&#39;: &#39;What does the text say about National Affairs Columnist, Chantal Hébert?&#39;}, {&#39;answer&#39;: &#34;The prime minister&#39;s capacity to forge a consensus in Parliament, on the federal-provincial front and his own caucus will be tested as some key campaign commitments face their deadlines. There are three potentially troublesome fronts for Trudeau&#39;s Liberal government. He must come up with a coherent response to the voting system. &#34;, &#39;question&#39;: &#39;What does the text say about Canadian Prime Minister Justin Trudeau?&#39;}, {&#39;answer&#39;: &#34;On or before December 1, the special committee exploring reform of Canada&#39;s voting system will report its findings to the government. Democratic reform minister Maryam Monsef made her own consultations and found that there was no consensus within the public as to a preferred voting system. On December 9, Trudeau is tentatively scheduled to meet with the premiers to put the finishing touches on the country&#39;s climate change strategy. &#34;, &#39;question&#39;: &#39;Summarize everything that happened between December 1 to December 9.&#39;}, {&#39;answer&#39;: &#34;December 19 is the deadline for the federal cabinet to decide the fate of Kinder Morgan&#39;s plan to increase the capacity of the TransMountain pipeline. The pipeline links Alberta to the coast of Vancouver. However, Trudeau has long said that he would not proceed with a pipeline absent a social license for the project.&#34;, &#39;question&#39;: &#39;What does the text say about December 19th?&#39;}] . . Look at the first question-answer pair . The question being asked does not have an answer in the accompanying text, thus the answer column is empty | . len_question_answer_pairs = df[&#39;data&#39;].apply(lambda x: len(x[&#39;questions&#39;])) len_question_answer_pairs . 0 4 1 5 2 5 3 5 4 6 .. 11227 5 11228 4 11229 4 11230 5 11231 6 Name: data, Length: 11232, dtype: int64 . len_question_answer_pairs.value_counts().sort_values().loc[[i for i in range(1,10)]].plot(kind=&#39;bar&#39;) plt.title(&#39;Question-Answer pairs by frequency&#39;) plt.xlabel(&#39;Number of Question-Answer Pairs&#39;) plt.ylabel(&#39;Frequency&#39;); .",
            "url": "https://kaykozaronek.github.io/blog/gpt-3/language%20models/ai%20safety/ai%20alignment/dataset/openai/2022/06/29/Exploring-OpenAI's-Critique-Dataset.html",
            "relUrl": "/gpt-3/language%20models/ai%20safety/ai%20alignment/dataset/openai/2022/06/29/Exploring-OpenAI's-Critique-Dataset.html",
            "date": " • Jun 29, 2022"
        }
        
    
  
    
        ,"post6": {
            "title": "Weekly Goals Update 01",
            "content": "Status: . Here&#39;s the progress of last weeks goals. . Goal Points Progress Total Points . 0 Finish reading Transformer Book | 30 | 1.000000 | 30.000000 | . 1 Read &quot;Self-critiquing models for assisting human evaluators&quot; | 20 | 1.000000 | 20.000000 | . 2 Read &quot;Learning to summarize from human feedback&quot; | 10 | 1.000000 | 10.000000 | . 3 Read &quot;Training Language Models with Language Feedback&quot; | 20 | 1.000000 | 20.000000 | . 4 Explore Critique Dataset in Notebook | 10 | 0.000000 | 0.000000 | . 5 Code REINFORCE | 5 | 1.000000 | 5.000000 | . 6 Code: A2C | 5 | 1.000000 | 5.000000 | . 7 Total | | | 90.000000 | . Goals: . This week is a little more scattered. I&#39;m helping out a friend who&#39;s created a course to test it and provide feedback. I also want to play around with different ideas of how I can make my CHERI project the most effective. I want it to be a great sample of my work, both in writing and coding, irrespective of the final outcome. That means that I should continuously create code and written artifacts that I will be able to present as a “portfolio”. Building a blog for this purpose seems like something worth considering. . Goal Points . 0 Reimplement &quot;Training Language Models with Lan... | 25 | . 1 Finish Software Engineering practices for Data... | 15 | . 2 Create blog | 25 | . 3 Apply to Future Forum | 10 | . 4 Apply to Future Academy | 5 | . 5 Code Proximal Policy Optimization (PPO) | 5 | . 6 Code Generalized Advantage Estimation (GAE) | 5 | . 7 Creat Github repo for CHERI project | 5 | . 8 Do CHERI project planning | 5 | . 9 Total | | . Explore in upcoming week(s): . Code Karpathy MinGPT | Code MLAB GPT | Deep RL Lecture Series (UC Berkeley) | NYU Pytorch Deep Learning | .",
            "url": "https://kaykozaronek.github.io/blog/reading/books/academic%20papers/blogs/mental%20diet/2022/06/26/Update-01-Weekly-Goals.html",
            "relUrl": "/reading/books/academic%20papers/blogs/mental%20diet/2022/06/26/Update-01-Weekly-Goals.html",
            "date": " • Jun 26, 2022"
        }
        
    
  
    
        ,"post7": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://kaykozaronek.github.io/blog/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website chronicles my exploration of learning in machines and humans. .",
          "url": "https://kaykozaronek.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://kaykozaronek.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}